{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKXFMhGUuwV49G5gaF4Wom",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ioannis-toumpoglou/pytorch-repo/blob/main/pytorch_custom_model_coco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aXhI-_rBmYiX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "\n",
        "class CocoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, annotation, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.coco = COCO(annotation)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Own coco file\n",
        "        coco = self.coco\n",
        "        # Image ID\n",
        "        img_id = self.ids[index]\n",
        "        # List: get annotation id from coco\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        # Dictionary: target coco_annotation file for an image\n",
        "        coco_annotation = coco.loadAnns(ann_ids)\n",
        "        # path for input image\n",
        "        path = coco.loadImgs(img_id)[0]['file_name']\n",
        "        # open the input image\n",
        "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
        "\n",
        "        # number of objects in the image\n",
        "        num_objs = len(coco_annotation)\n",
        "\n",
        "        # Bounding boxes for objects\n",
        "        # In coco format, bbox = [xmin, ymin, width, height]\n",
        "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            xmin = coco_annotation[i]['bbox'][0]\n",
        "            ymin = coco_annotation[i]['bbox'][1]\n",
        "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
        "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # Labels (In my case, I only one class: target class or background)\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        # Tensorise img_id\n",
        "        img_id = torch.tensor([img_id])\n",
        "        # Size of bbox (Rectangular)\n",
        "        areas = []\n",
        "        for i in range(num_objs):\n",
        "            areas.append(coco_annotation[i]['area'])\n",
        "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "        # Iscrowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Annotation is in dictionary format\n",
        "        my_annotation = {}\n",
        "        my_annotation[\"boxes\"] = boxes\n",
        "        my_annotation[\"labels\"] = labels\n",
        "        my_annotation[\"image_id\"] = img_id\n",
        "        my_annotation[\"area\"] = areas\n",
        "        my_annotation[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, my_annotation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The inputs for a PyTorch model must be in tensor format\n",
        "def get_transform():\n",
        "    custom_transforms = []\n",
        "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
        "    return torchvision.transforms.Compose(custom_transforms)"
      ],
      "metadata": {
        "id": "Z-vxYezJogsv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data directory\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "data_path = Path('data/')\n",
        "image_path = data_path / 'images'\n",
        "\n",
        "if image_path.is_dir():\n",
        "  print(f'[INFO] {image_path} already exists, skipping download...')\n",
        "else:\n",
        "  print(f'[INFO] Unable to find {image_path}, creating one...')\n",
        "  image_path.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkEWGv3nv0_e",
        "outputId": "c5621fdf-e6e0-4ea2-a6f9-3555cc94d7ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] data/images already exists, skipping download...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path to data and coco file\n",
        "train_data_dir = 'data/'\n",
        "train_coco = 'data/train_coco.json'\n",
        "\n",
        "# create own Dataset\n",
        "my_dataset = CocoDataset(root=train_data_dir,\n",
        "                         annotation=train_coco,\n",
        "                         transforms=get_transform()\n",
        "                         )\n",
        "\n",
        "# collate_fn needs for batch\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# Batch size\n",
        "train_batch_size = 32\n",
        "num_workers = os.cpu_count()\n",
        "\n",
        "# own DataLoader\n",
        "data_loader = torch.utils.data.DataLoader(my_dataset,\n",
        "                                          batch_size=train_batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=num_workers,\n",
        "                                          collate_fn=collate_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMBfELxhsLPv",
        "outputId": "5877ac85-1fc2-4ed9-f0c2-ad2e6da5f463"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select device (whether GPU or CPU)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# DataLoader is iterable over Dataset\n",
        "for imgs, annotations in data_loader:\n",
        "    imgs = list(img.to(device) for img in imgs)\n",
        "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "    print(annotations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um05XOh5td5e",
        "outputId": "d18e3805-b020-42cc-b6e0-120b1638c9f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'boxes': tensor([[ 573.8427,  149.5730, 1074.3370,  408.4494]]), 'labels': tensor([1]), 'image_id': tensor([5]), 'area': tensor([129566.1875]), 'iscrowd': tensor([0])}, {'boxes': tensor([[  32.1857,   34.6615, 1245.3385, 1245.3385]]), 'labels': tensor([1]), 'image_id': tensor([2]), 'area': tensor([1468736.1250]), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 111.4120,  106.4604, 1173.5397, 1252.7660]]), 'labels': tensor([1]), 'image_id': tensor([3]), 'area': tensor([1217522.8750]), 'iscrowd': tensor([0])}, {'boxes': tensor([[471.3017, 267.6731, 748.8279, 558.3365]]), 'labels': tensor([1]), 'image_id': tensor([7]), 'area': tensor([80666.6953]), 'iscrowd': tensor([0])}, {'boxes': tensor([[   1.5725,    3.1451, 1278.4700,  809.8549]]), 'labels': tensor([1]), 'image_id': tensor([6]), 'area': tensor([1030085.8125]), 'iscrowd': tensor([0])}, {'boxes': tensor([[460.3230, 448.7737, 821.6519, 595.6151]]), 'labels': tensor([1]), 'image_id': tensor([12]), 'area': tensor([53058.0273]), 'iscrowd': tensor([0])}, {'boxes': tensor([[   7.4275,   44.5648, 1275.0483, 1242.8627]]), 'labels': tensor([1]), 'image_id': tensor([8]), 'area': tensor([1518987.3750]), 'iscrowd': tensor([0])}, {'boxes': tensor([[722.3211, 291.5280, 972.9980, 571.9149]]), 'labels': tensor([1]), 'image_id': tensor([1]), 'area': tensor([70286.5312]), 'iscrowd': tensor([0])}, {'boxes': tensor([[348.0450, 192.7191, 893.1236, 514.8764]]), 'labels': tensor([1]), 'image_id': tensor([4]), 'area': tensor([175601.0625]), 'iscrowd': tensor([0])}, {'boxes': tensor([[504.2785, 235.6596, 695.4429, 339.4816],\n",
            "        [693.7950, 557.0135, 807.5048, 617.9884]]), 'labels': tensor([1, 1]), 'image_id': tensor([11]), 'area': tensor([19847.0801,  6933.4424]), 'iscrowd': tensor([0, 0])}, {'boxes': tensor([[ 97.5470,  43.3542, 210.9349,  95.8795],\n",
            "        [451.8843, 159.2434, 524.4193, 190.0916],\n",
            "        [611.1277, 291.8072, 688.6650, 319.3205]]), 'labels': tensor([1, 1, 1]), 'image_id': tensor([14]), 'area': tensor([5955.7363, 2237.5718, 2133.3047]), 'iscrowd': tensor([0, 0, 0])}, {'boxes': tensor([[371.0562, 279.0112, 824.0899, 490.4270]]), 'labels': tensor([1]), 'image_id': tensor([0]), 'area': tensor([95778.4531]), 'iscrowd': tensor([0])}, {'boxes': tensor([[205.6980,  23.9015, 307.0985,  65.1860]]), 'labels': tensor([1]), 'image_id': tensor([16]), 'area': tensor([4186.2627]), 'iscrowd': tensor([0])}, {'boxes': tensor([[  11.9304,    9.9420, 1274.5609, 1014.0812]]), 'labels': tensor([1]), 'image_id': tensor([19]), 'area': tensor([1267856.8750]), 'iscrowd': tensor([0])}, {'boxes': tensor([[  14.8549,   24.7582, 1267.6208, 1242.8627]]), 'labels': tensor([1]), 'image_id': tensor([10]), 'area': tensor([1525999.7500]), 'iscrowd': tensor([0])}, {'boxes': tensor([[  1.7215,   4.4758, 266.4836, 173.8685]]), 'labels': tensor([1]), 'image_id': tensor([13]), 'area': tensor([44848.7500]), 'iscrowd': tensor([0])}, {'boxes': tensor([[  49.3494,   46.2651, 1229.1084,  601.4458]]), 'labels': tensor([1]), 'image_id': tensor([18]), 'area': tensor([654979.5000]), 'iscrowd': tensor([0])}, {'boxes': tensor([[321.2288, 112.7119, 387.2458, 146.5254]]), 'labels': tensor([1]), 'image_id': tensor([17]), 'area': tensor([2232.2681]), 'iscrowd': tensor([0])}, {'boxes': tensor([[  14.8549,   37.1373, 1262.6692, 1252.7660]]), 'labels': tensor([1]), 'image_id': tensor([9]), 'area': tensor([1516878.7500]), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 24.1973,  30.6499, 187.1257, 101.6286],\n",
            "        [ 24.1973, 132.2785, 187.1257, 203.2572],\n",
            "        [ 25.0039, 230.6808, 187.1257, 302.4662],\n",
            "        [ 24.1973, 329.8897, 187.1257, 401.6750],\n",
            "        [225.8414, 330.6963, 387.9633, 401.6750],\n",
            "        [224.2282, 230.6808, 387.9633, 302.4662],\n",
            "        [225.0348, 129.8588, 387.9633, 204.0638],\n",
            "        [225.0348,  31.4565, 387.9633, 102.4352]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1]), 'image_id': tensor([15]), 'area': tensor([11564.4521, 11564.4521, 11637.9658, 11695.8662, 11507.2021, 11753.7666,\n",
            "        12090.1094, 11564.4521]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0])}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=False)\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = fasterrcnn_resnet50_fpn_v2(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# 2 classes; Only target class or background\n",
        "num_classes = 2\n",
        "num_epochs = 10\n",
        "model = get_model_instance_segmentation(num_classes)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# parameters\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "len_dataloader = len(data_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    i = 0\n",
        "    for imgs, annotations in data_loader:\n",
        "        i += 1\n",
        "        imgs = list(img.to(device) for img in imgs)\n",
        "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "        loss_dict = model(imgs, annotations)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXk6W-YPte3V",
        "outputId": "753779be-22c9-4aae-f194-9635bb0fb25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' and 'progress' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n",
            "100%|██████████| 167M/167M [00:02<00:00, 80.4MB/s]\n"
          ]
        }
      ]
    }
  ]
}